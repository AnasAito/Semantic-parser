{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import krovetz\n",
    "\n",
    "## utils\n",
    "import urllib.request as libreq\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re, string\n",
    "import networkx as nx\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "from random import random\n",
    "\n",
    "ks = krovetz.PyKrovetzStemmer()\n",
    "def get_shemantic_paper_html(where):\n",
    "    '''\n",
    "    get page html using filters on citation list\n",
    "    \n",
    "    arguments : paper href \n",
    "    output : paper html \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    base_url = 'https://www.semanticscholar.org'\n",
    "    filter_ = '?citationRankingModelVersion=v0.2.0-0.01&citedPapersSort=relevance&citedPapersLimit=10&citedPapersOffset=0&sort=total-citations'\n",
    "    URL = base_url+where+filter_\n",
    "    \n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    return soup \n",
    "def extract_data(soup):\n",
    "    ## init \n",
    "    data = {}\n",
    "    cits_list=[]\n",
    "    refs_list = []\n",
    "    ## genral info \n",
    "    data[\"title\"] = soup.find('h1' , {'data-selenium-selector':'paper-detail-title'}).text\n",
    "    data['corpus_id'] = soup.find('span' , {'data-selenium-selector':'corpus-id'}).text\n",
    "    data[\"additional_data\"] = soup.find('span' , {'data-selenium-selector':'year-and-venue'}).text\n",
    "    \n",
    "    ## citations\n",
    "    score_card = soup.find('span' , {'class':'scorecard-stat__headline'})\n",
    "    if score_card : \n",
    "        \n",
    "        data[\"citation_count\"] = score_card.text.split(\" \")[0]\n",
    "        \n",
    "        ## citations type\n",
    "        citations_count = [div.text for div in  soup.find_all('div' , {'class':'scorecard-citation__metadata-item'})]\n",
    "        citations_title = [div.text for div in soup.find_all('div' , {'class':'scorecard-citation__title'})]\n",
    "        if len(citations_title)< len(citations_count) :\n",
    "            citations_title.insert(0,'Highly Influencial Citations')\n",
    "            \n",
    "        data['citations_overview'] = {\"cit_titles\":citations_title , \"cit_count\" : citations_count }\n",
    "        \n",
    "    else : \n",
    "        data[\"citation_count\"] = ''\n",
    "        data['citations_overview'] ={}\n",
    "\n",
    "    ## paper topics \n",
    "    topic_section = soup.find('div',{'data-selenium-selector':'entities-list'})\n",
    "    is_topics = topic_section is not None\n",
    "    \n",
    "    if is_topics : \n",
    "        data['topics'] = [span.text for span in topic_section.find_all('span',{'class' :'preview-box__target' })]\n",
    "    else : \n",
    "        data['topics'] = []\n",
    "    \n",
    "    ## main citations , refs \n",
    "    cards = soup.find_all('div', class_='cl-paper-row citation-list__paper-row')\n",
    "    citations_cards = cards[:10]\n",
    "    refs_cards = cards [10:]\n",
    "\n",
    "    for cit in citations_cards : \n",
    "        entry = {}\n",
    "        entry['title'] = cit.find('div' , class_='cl-paper-title').text\n",
    "        entry['link'] = cit.find('a')['href']\n",
    "        \n",
    "        stats_raw = cit.find('div',class_='cl-paper-controls__stats')#.find_all('div',class_='cl-paper-stat') \n",
    "        if stats_raw :\n",
    "            stats = [div.text for div in stats_raw.find_all('div',class_='cl-paper-stat') ]\n",
    "            #print(stats)\n",
    "            entry['stats']=stats\n",
    "        else : \n",
    "            entry['stats']=[]\n",
    "\n",
    "\n",
    "        cits_list.append(entry)\n",
    "    for ref in refs_cards : \n",
    "        entry = {}\n",
    "        entry['title'] = ref.find('div' , class_='cl-paper-title').text\n",
    "        entry['link'] = ref.find('a')['href']\n",
    "        \n",
    "        stats_raw = ref.find('div',class_='cl-paper-controls__stats')#.find_all('div',class_='cl-paper-stat') \n",
    "        if stats_raw :\n",
    "            stats = [div.text for div in stats_raw.find_all('div',class_='cl-paper-stat') ]\n",
    "            #print(stats)\n",
    "            entry['stats']=stats\n",
    "        else : \n",
    "            entry['stats']=[]\n",
    "\n",
    "\n",
    "        refs_list.append(entry)\n",
    "        \n",
    "            \n",
    "\n",
    "    data['citations'] = cits_list\n",
    "    data['references'] = refs_list\n",
    "    \n",
    "    ## return data dict \n",
    "    return data \n",
    "\n",
    "def ravel(list_):return [j for sub in list_ for j in sub]\n",
    " \n",
    "def generate_wcloud(topics_list,max_words=100,stopwords_list =['learning','neural' , 'computer','algorithm','network','Artificial','model']):\n",
    "    '''\n",
    "    args : 1d topics list (1 x n)\n",
    "    output : word cloud image \n",
    "    '''\n",
    "    text = ' '.join(topics_list)\n",
    "    \n",
    "    # lower max_font_size, change the maximum number of word and lighten the background:\n",
    "    stopwords = set(stopwords_list)\n",
    "    wordcloud = WordCloud(max_font_size=100,stopwords=stopwords, max_words=max_words, background_color=\"white\" ).generate(text)\n",
    "    print(wordcloud)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def get_semantic_scholar_paper_by_api(paper_id , is_external =False ,citationType='relevance') : \n",
    "    \n",
    "    b_url_from_arxiv = 'https://api.semanticscholar.org/v1/paper/arXiv:'#'hep-ph/0610184'\n",
    "    b_url = 'https://api.semanticscholar.org/v1/paper/'#'0796f6cd7f0403a854d67d525e9b32af3b277331'\n",
    "\n",
    "\n",
    "    final_url = is_external*(b_url_from_arxiv+paper_id) + (not is_external )*(b_url+paper_id)\n",
    "    print(final_url)\n",
    "    with libreq.urlopen(final_url) as url:\n",
    "         r = url.read()\n",
    "    data = json.loads(r)\n",
    "    #print(data)\n",
    "    return data        \n",
    "\n",
    "\n",
    "def load_file(filename):\n",
    "\n",
    "    with open(filename,'r') as f:\n",
    "        text = f.readlines()\n",
    "    return text\n",
    "\n",
    "def preprocess(line):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    line = [item.lower() for item in line if not item.lower() in stop_words]\n",
    "    #print(line)\n",
    "    stemmed_line = [ks.stem(item) for item in line]\n",
    "    #print(stemmed_line)\n",
    "    return stemmed_line\n",
    "\n",
    "def create_graph(text):\n",
    "    text = [text.translate(str.maketrans('', '', string.punctuation))]\n",
    "    word_list = []\n",
    "    G = nx.Graph()\n",
    "    #pbar = tqdm(total=len(text))\n",
    "    for line in text:\n",
    "        #print(line)\n",
    "        line = (line.strip()).split()\n",
    "        line = preprocess(line)\n",
    "        #print(line)\n",
    "        for i, word in enumerate(line):\n",
    "            if i != len(line)-1:\n",
    "                word_a = word\n",
    "                word_b = line[i+1]\n",
    "                #print(word_a,word_b)\n",
    "                if word_a not in word_list:\n",
    "                    word_list.append(word_a)\n",
    "                    \n",
    "                if word_b not in word_list:\n",
    "                    word_list.append(word_b)\n",
    "                    \n",
    "                if G.has_edge(word_a,word_b):\n",
    "                    G[word_a][word_b]['weight'] += 1\n",
    "                    G[word_a][word_b]['distance']  = 1/G[word_a][word_b]['weight'] \n",
    "                    #print(G.nodes)\n",
    "                else:\n",
    "                    G.add_edge(word_a,word_b, weight = 1)\n",
    "                    G.add_edge(word_a,word_b, distance = 1)\n",
    "                    #print(G.nodes)\n",
    "      #  pbar.update(1)\n",
    "    #pbar.close()\n",
    "    return G\n",
    "\n",
    "\n",
    "def calculate_central_nodes(text_network , max_nodes = -1):\n",
    "\n",
    "    bc = (nx.betweenness_centrality(text_network,weight='weight'))\n",
    "    #print(bc)\n",
    "    \n",
    "    nx.set_node_attributes(text_network, bc, 'betweenness')\n",
    "    bc_threshold = sorted(bc.values(), reverse=True)[max_nodes]\n",
    "    to_keep = [n for n in bc if bc[n] > bc_threshold]\n",
    "    filtered_network = text_network.subgraph(to_keep)\n",
    "    return filtered_network\n",
    "def plot_betweenness_centrality(text_network,max_nodes):\n",
    "    bc = (nx.betweenness_centrality(text_network,weight='weight'))\n",
    "    sorted_bc = dict(sorted(bc.items(),reverse=True, key=lambda item: item[1]))\n",
    "    nodes = list(sorted_bc.keys())[:max_nodes]\n",
    "    values = list(sorted_bc.values())[:max_nodes]\n",
    "    fig = px.bar( x=nodes, y=values)\n",
    "    fig.show()\n",
    "def plot_degree_centrality(text_network,max_nodes , normalized =True):\n",
    "    if normalized : \n",
    "        bc = (nx.degree_centrality(text_network))\n",
    "    else : \n",
    "        bc = dict(text_network.degree())\n",
    "    #print(bc)\n",
    "    sorted_bc = dict(sorted(bc.items(),reverse=True, key=lambda item: item[1]))\n",
    "    nodes = list(sorted_bc.keys())[:max_nodes]\n",
    "    values = list(sorted_bc.values())[:max_nodes]\n",
    "    fig = px.bar( x=nodes, y=values)\n",
    "    fig.show()\n",
    "def scatter_centralities(text_network) :\n",
    "    deg = (nx.degree_centrality(text_network))\n",
    "    bc = (nx.betweenness_centrality(text_network,weight='weight'))\n",
    "    values_bc =  list(bc.values())\n",
    "    values_deg =  list(deg.values())\n",
    "    labels = list(bc.keys())\n",
    "   # print(labels)\n",
    "    fig = px.scatter( x=values_deg, y=values_bc,labels = labels)\n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "def create_and_assign_communities(text_network):\n",
    "\n",
    "    communities_generator = community.girvan_newman(text_network)\n",
    "    top_level_communities = next(communities_generator)\n",
    "    next_level_communities = next(communities_generator)\n",
    "    communities = {}\n",
    "    for community_list in next_level_communities:\n",
    "        for item in community_list:\n",
    "            communities[item] = next_level_communities.index(community_list)\n",
    "    return communities\n",
    "\n",
    "def draw_final_graph(text_network,communities ,with_size=True ):\n",
    "\n",
    "    pos = nx.spring_layout(text_network,scale=2)\n",
    "    color_list = []\n",
    "    color_map = []\n",
    "    community_count = max(communities.values())\n",
    "    for i in range(0,community_count+1):\n",
    "        color_list.append((random(), random(), random()))\n",
    "    for node in text_network:\n",
    "        color_index = communities[node]\n",
    "        color_map.append(color_list[color_index])\n",
    "    betweenness = nx.get_node_attributes(text_network,'betweenness')\n",
    "    betweenness = [x*1000 for x in betweenness.values()]\n",
    "    if with_size : \n",
    "         nx.draw(text_network,with_labels=True,node_size=betweenness,font_size=8,node_color=color_map,edge_cmap=plt.cm.Blues)\n",
    "    else :  nx.draw(text_network,with_labels=True,font_size=10,node_color=color_map,edge_cmap=plt.cm.Blues)\n",
    "    plt.draw()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to do \n",
    "## parse abstract directly (query to api take too much time) use selenium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "None is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
